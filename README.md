# mini-transformer ðŸ§ âš¡

A from-scratch **seq2seq Transformer** (PyTorch) that learns a simple but demonstrative task:
**reverse a string**. Itâ€™s small, readable, and production-ishâ€”perfect for showcasing code quality
to recruiters while being quick to run locally or in a Colab.

## Why this repo might impress
- âœ¨ Clean, typed, human-friendly code (no magic â€œblack boxâ€).
- ðŸ§© From-scratch attention, encoder/decoder blocks, positional encodings.
- ðŸ§ª Tests (pytest), linting, CI (GitHub Actions), Dockerfile, and Makefile.
- ðŸ› ï¸ Simple CLI to train/eval/infer. No massive datasets required.
- ðŸ“¦ Reusable tokenizer and datamodule (byte-level with PAD/BOS/EOS).

> Task: Given a string like `hello123`, predict `321olleh`

---

## Quickstart

### 1) Install
```bash
python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt
```

### 2) Train (CPU okay)
```bash
python scripts/train.py --epochs 3 --batch-size 128
```

### 3) Evaluate
```bash
python scripts/eval.py
```

### 4) Inference
```bash
python scripts/infer.py --text "ChatGPT-5 is awesome!" --max-new-tokens 64
# -> prints the reversed string
```

### 5) Colab
Open `notebooks/colab_quickstart.ipynb` (minimal).

---

## Repo Layout

```
.
â”œâ”€â”€ src/transformer_tiny
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # dataclasses for hyperparams
â”‚   â”œâ”€â”€ tokenizer.py       # byte-level tokenizer (PAD/BOS/EOS)
â”‚   â”œâ”€â”€ data.py            # dataset + dataloaders (reverse task)
â”‚   â”œâ”€â”€ model.py           # positional enc, attention, encoder/decoder
â”‚   â”œâ”€â”€ utils.py           # seed, checkpoints, metrics, masks
â”‚   â””â”€â”€ train_eval.py      # train/eval steps
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ train.py           # CLI training
â”‚   â”œâ”€â”€ eval.py            # CLI evaluation
â”‚   â””â”€â”€ infer.py           # CLI greedy decoding
â”œâ”€â”€ tests                  # small unit tests
â”‚   â”œâ”€â”€ test_tokenizer.py
â”‚   â””â”€â”€ test_model_shapes.py
â”œâ”€â”€ notebooks
â”‚   â””â”€â”€ colab_quickstart.ipynb
â”œâ”€â”€ .github/workflows/ci.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml         # black/isort/flake8 settings
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## Example Results

After ~3 epochs on CPU, the model should reliably learn to reverse short strings.

```
input : "Hello_2025"
target: "5202_olleH"
pred  : "5202_olleH"
```

---

## Notes

- This is a **teaching/portfolio** repo: readable code > maximal speed.
- Easily adapt `data.py` to new toy tasks (e.g., copy, shift, parentheses balancing).
- For real tasks, hook in a proper dataset/tokenizer and scale the config.

**MIT License** â€” Have fun!

â€” Built 2025-10-19