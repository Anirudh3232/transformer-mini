# mini-transformer

A from-scratch **seq2seq Transformer** (PyTorch) that learns a simple but demonstrative task:
**reverse a string**. Itâ€™s small, readable, and production-ishâ€”perfect.
.

- ðŸ§©From-scratch attention, encoder/decoder blocks, positional encodings.
-  Reusable tokenizer and datamodule (byte-level with PAD/BOS/EOS).

> Task: Given a string like `hello123`, predict `321olleh`
## Quickstart

### 1) Install
```bash
python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install -r requirements.txt
```

### 2) Train
```bash
python scripts/train.py --epochs 3 --batch-size 128
```

### 3) Evaluate
```bash
python scripts/eval.py
```

### 4) Inference
```bash
python scripts/infer.py --text "Going good!" --max-new-tokens 64
# -> prints the reversed string
```

### 5) Colab
Open `notebooks/colab_quickstart.ipynb` (minimal).

---

## Repo Layout
```
.
â”œâ”€â”€ src/transformer_tiny
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # dataclasses for hyperparams
â”‚   â”œâ”€â”€ tokenizer.py       # byte-level tokenizer (PAD/BOS/EOS)
â”‚   â”œâ”€â”€ data.py            # dataset + dataloaders (reverse task)
â”‚   â”œâ”€â”€ model.py           # positional enc, attention, encoder/decoder
â”‚   â”œâ”€â”€ utils.py           # seed, checkpoints, metrics, masks
â”‚   â””â”€â”€ train_eval.py      # train/eval steps
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ train.py           # CLI training
â”‚   â”œâ”€â”€ eval.py            # CLI evaluation
â”‚   â””â”€â”€ infer.py           # CLI greedy decoding
â”œâ”€â”€ tests                  # small unit tests
â”‚   â”œâ”€â”€ test_tokenizer.py
â”‚   â””â”€â”€ test_model_shapes.py
â”œâ”€â”€ notebooks
â”‚   â””â”€â”€ colab_quickstart.ipynb
â”œâ”€â”€ .github/workflows/ci.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml         # black/isort/flake8 settings
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```
input : "Hello_2025"
target: "5202_olleH"
pred  : "5202_olleH"
```

---

## Notes
- Easily adapt `data.py` to new toy tasks (e.g., copy, shift, parentheses balancing).
- For real tasks, hook in a proper dataset/tokenizer and scale the config.

**MIT License** 
